#include <gpd/net/openvino_classifier.h>

namespace gpd {
namespace net {

using namespace InferenceEngine;

std::map<Classifier::Device, InferenceEngine::TargetDevice>
    OpenVinoClassifier::device_map_ = {
        {Classifier::Device::eCPU, TargetDevice::eCPU},
        {Classifier::Device::eGPU, TargetDevice::eGPU},
        {Classifier::Device::eVPU, TargetDevice::eMYRIAD},
        {Classifier::Device::eFPGA, TargetDevice::eFPGA}};

OpenVinoClassifier::OpenVinoClassifier(const std::string& model_file,
                                       const std::string& weights_file,
                                       Classifier::Device device,
                                       int batch_size) {
  // Load MKLDNN Plugin for Inference Engine.
  InferenceEngine::PluginDispatcher dispatcher({"../../../lib/intel64", ""});
  plugin_ = InferencePlugin(dispatcher.getSuitablePlugin(device_map_[device]));
  // Load IR Generated by ModelOptimizer (.xml and .bin files)
  CNNNetReader network_reader;
  /*  switch (device) {
      case Classifier::Device::eCPU:
        network_reader.ReadNetwork(std::string(MODELS_DIR) +
                                   "/fp32/single_view_15_channels.xml");
        network_reader.ReadWeights(std::string(MODELS_DIR) +
                                   "/fp32/single_view_15_channels.bin");
        break;
      case Classifier::Device::eVPU:
        network_reader.ReadNetwork(std::string(MODELS_DIR) +
                                   "/fp16/single_view_15_channels.xml");
        network_reader.ReadWeights(std::string(MODELS_DIR) +
                                   "/fp16/single_view_15_channels.bin");
        break;
      case Classifier::Device::eGPU:
        std::cout << "GPU device to be supported!!\n";
        // fall through;
      case Classifier::Device::eFPGA:
        std::cout << "FPGA device to be supported!!\n";
        // fall through;
      default:
        throw std::exception();
    }*/
  network_reader.ReadNetwork(model_file);
  network_reader.ReadWeights(weights_file);
  network_ = network_reader.getNetwork();
  network_.setBatchSize(batch_size);

  // Prepare input blobs.
  auto input_info = network_.getInputsInfo().begin()->second;
  auto input_name = network_.getInputsInfo().begin()->first;
  input_info->setPrecision(Precision::FP32);

  // Prepare output blobs.
  auto output_info = network_.getOutputsInfo().begin()->second;
  auto output_name = network_.getOutputsInfo().begin()->first;
  output_info->setPrecision(Precision::FP32);

  // Load model into the plugin.
  printf("network input: %s, output: %s\n", input_name.c_str(),
         output_name.c_str());
  infer_request_ = plugin_.LoadNetwork(network_, {}).CreateInferRequest();
  input_blob_ = infer_request_.GetBlob(input_name);
  output_blob_ = infer_request_.GetBlob(output_name);

  // Prepare the input blob buffer.
  auto input_data =
      input_blob_->buffer().as<PrecisionTrait<Precision::FP32>::value_type*>();
  batch_image_list_.clear();
  for (int i = 0; i < input_blob_->dims()[2]; ++i) {
    cv::Mat img(input_blob_->dims()[0], input_blob_->dims()[1], CV_32FC1,
                input_data);
    batch_image_list_.push_back(img);
    input_data += input_blob_->dims()[0] * input_blob_->dims()[1];
  }
}

std::vector<float> OpenVinoClassifier::classifyImages(
    const std::vector<std::unique_ptr<cv::Mat>>& image_list) {
  std::vector<float> predictions;
  std::cout << "# images: " << image_list.size() << "\n";

  for (int i = 0; i < image_list.size(); i++) {
    cv::Mat img;
    image_list[i]->convertTo(img, CV_32FC(input_blob_->dims()[2]));
    cv::split(img, batch_image_list_);

    // Run inference request synchronously.
    infer_request_.Infer();

    // Post-process output blobs.
    auto output_data = output_blob_->buffer()
                           .as<PrecisionTrait<Precision::FP32>::value_type*>();
    // printf("positive score: %3.4f, negative score: %3.4f\n");
    predictions.push_back(output_data[1] - output_data[0]);
  }

  return predictions;
}

int OpenVinoClassifier::getBatchSize() const { return network_.getBatchSize(); }

}  // namespace net
}  // namespace gpd
